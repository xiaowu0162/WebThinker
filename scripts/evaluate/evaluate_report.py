# from prompts_report import get_report_evaluation_instruction
import json
import os
import random
from pathlib import Path
import time
import asyncio
from tqdm import tqdm
from openai import OpenAI

API_BASE_URL = "https://api.deepseek.com"
MODEL_NAME = "deepseek-reasoner"  # deepseek-chat, deepseek-reasoner
API_KEY = "YOUR_DEEPSEEK_API"

client = OpenAI(
    api_key=API_KEY,
    base_url=API_BASE_URL,
)

test_path = "./data/Glaive/test.json"
naive_rag_dir = "./outputs/Glaive.Qwen2.5-72B-Instruct.naive_rag/markdown.test.3.28,20:55.94"
webthinker_dir = "./outputs/glaive.qwq.webthinker/markdown.test.3.27,21:47.41"
gemini_dir = "./outputs/glaive.Gemini.DeepResearch"
grok3_dir = "./outputs/glaive.Grok3.DeeperSearch"





def get_report_evaluation_instruction(question, system_a, system_b, system_c, system_d):
    return f"""Research Question: {question}

Please objectively evaluate the quality of research articles generated by systems A, B, C and D for this question, and provide scores out of 10 for the following criteria:
(1) Overall Comprehensiveness: The report should cover content as comprehensively as possible
(2) Thoroughness of Discussion: Each section should be discussed thoroughly, not just superficially
(3) Factuality: There should be minimal factual errors
(4) Coherence: The discussion should stay focused and relevant to the topic

Notes:
- A satisfactory performance deserves around 5 points, with higher scores for excellence and lower scores for deficiencies
- You should not easily assign scores higher than 8 or lower than 3 unless you provide substantial reasoning.
- You do not need to consider citations in the articles


----------------------------------------------------------
Research article generated by system A:
----------------------------------------------------------

{system_a}

----------------------------------------------------------



----------------------------------------------------------
Research article generated by system B:
----------------------------------------------------------

{system_b}

----------------------------------------------------------



----------------------------------------------------------
Research article generated by system C:
----------------------------------------------------------

{system_c}

----------------------------------------------------------



----------------------------------------------------------
Research article generated by system D:
----------------------------------------------------------

{system_d}

----------------------------------------------------------



Research Question: {question}

Please objectively evaluate the quality of research articles generated by systems A, B, C and D for this question, and provide scores out of 10 for the following criteria:
(1) Overall Comprehensiveness: The report should cover content as comprehensively as possible
(2) Thoroughness of Discussion: Each section should be discussed thoroughly, not just superficially
(3) Factuality: There should be minimal factual errors
(4) Coherence: The discussion should stay focused and relevant to the topic

Notes:
- A satisfactory performance deserves around 5 points, with higher scores for excellence and lower scores for deficiencies
- You should not easily assign scores higher than 8 or lower than 3 unless you provide substantial reasoning.
- You do not need to consider citations in the articles


Please analyze each article and provide the final scores in the following JSON format:

```json
{{
  "System A": {{
    "Overall Comprehensiveness": ,
    "Thoroughness of Discussion": ,
    "Factuality": ,
    "Coherence": 
  }},
  "System B": {{
    "Overall Comprehensiveness": ,
    "Thoroughness of Discussion": ,
    "Factuality": ,
    "Coherence": 
  }},
  "System C": {{
    "Overall Comprehensiveness": ,
    "Thoroughness of Discussion": ,
    "Factuality": ,
    "Coherence": 
  }},
  "System D": {{
    "Overall Comprehensiveness": ,
    "Thoroughness of Discussion": ,
    "Factuality": ,
    "Coherence": 
  }}
}}
```
"""

# Function to read markdown file content
def read_md_file(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
        content = content.split("#### **Works cited**")[0].split("#### Key Citations")[0].strip('\n').strip()
        return content

# Function to read test questions
def read_test_questions(test_path):
    with open(test_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return [item["Question"] for item in data]

# Function to extract scores from evaluation response
def extract_scores(response_text):
    try:
        # Find the JSON block in the response
        start = response_text.find('{')
        end = response_text.rfind('}') + 1
        json_str = response_text[start:end]
        scores = json.loads(json_str)
        
        return scores
    except:
        print("Failed to parse JSON from response")
        return None


# Initialize score tracking
system_scores = {
    "naive_rag": {"Comprehensiveness": [], "Thoroughness": [], "Factuality": [], "Coherence": []},
    "webthinker": {"Comprehensiveness": [], "Thoroughness": [], "Factuality": [], "Coherence": []},
    "gemini": {"Comprehensiveness": [], "Thoroughness": [], "Factuality": [], "Coherence": []},
    "grok3": {"Comprehensiveness": [], "Thoroughness": [], "Factuality": [], "Coherence": []}
}

# 添加一个新的字典来存储每个问题的具体评分
detailed_scores = []

# Read test questions
questions = read_test_questions(test_path)

# Process each article
for i in tqdm(range(30)):
    article_num = i + 1
    
    # Read articles from each system
    articles = {
        "naive_rag": read_md_file(os.path.join(naive_rag_dir, f"article_{article_num}.md")),
        "webthinker": read_md_file(os.path.join(webthinker_dir, f"article_{article_num}.md")),
        "gemini": read_md_file(os.path.join(gemini_dir, f"article_{article_num}.md")),
        "grok3": read_md_file(os.path.join(grok3_dir, f"article_{article_num}.md"))
    }
    
    # Randomly assign systems to A,B,C,D
    systems = list(articles.keys())
    random.shuffle(systems)
    system_mapping = {f"System {chr(65+i)}": system for i, system in enumerate(systems)}
    
    # Get evaluation instruction
    instruction = get_report_evaluation_instruction(
        question=questions[i],
        system_a=articles[system_mapping["System A"]],
        system_b=articles[system_mapping["System B"]], 
        system_c=articles[system_mapping["System C"]],
        system_d=articles[system_mapping["System D"]]
    )
    
    # Get evaluation from API
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": instruction}]
    )
    
    # Extract scores
    scores = extract_scores(response.choices[0].message.content)
    if scores:
        # 保存当前问题的详细评分
        question_detail = {
            "question_id": article_num,
            "question": questions[i],
            "scores": {}
        }
        
        # Map scores back to original systems
        for system_letter, scores_dict in scores.items():
            original_system = system_mapping[system_letter]
            system_scores[original_system]["Comprehensiveness"].append(scores_dict["Overall Comprehensiveness"])
            system_scores[original_system]["Thoroughness"].append(scores_dict["Thoroughness of Discussion"])
            system_scores[original_system]["Factuality"].append(scores_dict["Factuality"])
            system_scores[original_system]["Coherence"].append(scores_dict["Coherence"])
            
            # 为当前问题添加系统评分
            question_detail["scores"][original_system] = {
                "Overall Comprehensiveness": scores_dict["Overall Comprehensiveness"],
                "Thoroughness of Discussion": scores_dict["Thoroughness of Discussion"],
                "Factuality": scores_dict["Factuality"],
                "Coherence": scores_dict["Coherence"]
            }
        
        detailed_scores.append(question_detail)

# Calculate averages
final_scores = {}
for system, scores in system_scores.items():
    final_scores[system] = {
        metric: sum(values)/len(values) 
        for metric, values in scores.items()
    }

# Save results with timestamp
t = time.localtime()
timestamp = f"{t.tm_mon}.{t.tm_mday},{t.tm_hour}:{t.tm_min}.{t.tm_sec}"
output_path = os.path.join(webthinker_dir, f"evaluation_scores.{timestamp}.json")
with open(output_path, 'w') as f:
    json.dump(final_scores, f, indent=4)

# 保存详细结果
detailed_output_path = os.path.join(webthinker_dir, f"evaluation_scores_detailed.{timestamp}.json")
with open(detailed_output_path, 'w') as f:
    json.dump(detailed_scores, f, indent=4)

print("Evaluation complete. Results saved to:", output_path)
print("Detailed results saved to:", detailed_output_path)
print(final_scores)
